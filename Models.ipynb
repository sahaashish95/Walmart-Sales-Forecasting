{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                                    Case Study\n",
    "<hr style=\"border: 4px solid black;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](theme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problem Statement</h2>\n",
    "<hr style=\"border: 1px solid black;\" />\n",
    "\n",
    "\n",
    "This is kaggle competition problem .in this problem we have given historical sales data for 45 Walmart stores located in different regions. Each store contains many departments, and participants must project the sales for each department in each store. To add to the challenge, selected holiday markdown events are included in the dataset. These markdowns are known to affect sales, but it is challenging to predict which departments are affected and the extent of the impact.For more information click on this below link <br>\n",
    "https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/overview/evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problem Evaluation</h2>\n",
    "<hr style=\"border: 1px solid black;\" />\n",
    "\n",
    "\n",
    "This Problem is evaluated on the weighted mean absolute error (WMAE):\n",
    "\n",
    "                       WMAE=1∑wi∑i=1nwi|yi−y^i|\n",
    "where\n",
    "\n",
    "n is the number of rows<br>\n",
    "y^i is the predicted sales<br>\n",
    "yi is the actual sales<br>\n",
    "wi are weights. w = 5 if the week is a holiday week, 1 otherwise\n",
    "\n",
    "![title](sub.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Submission score criteria</h1>\n",
    "<hr style=\"border: 1px solid black;\" />\n",
    "\n",
    "![title](Submission1.png)\n",
    "\n",
    "There are total 688 submission of this problem and according to criteria of acceptence of case study is our submission score should be under top 10%\n",
    "\n",
    "![title](Submission.png)\n",
    "last 10% submisson score is 2850,so our score should be under 2850"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>importing all the libraries</h3>\n",
    "<hr style=\"border: 1px solid black;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#import lightgbm as lgb \n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# The current ver  sion of seaborn generates a bunch of warnings that we'll ignore\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style('darkgrid')\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "import pendulum\n",
    "\n",
    "\n",
    "#from xgboost import XGBRegressor\n",
    "import pickle\n",
    "from pandas import to_datetime\n",
    "import lightgbm \n",
    " \n",
    "#from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading a csv file\n",
    "feat=pd.read_csv('features.csv')\n",
    "store=pd.read_csv('stores.csv')\n",
    "sales_train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "sales_test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "43e18895-c900-45b7-a701-dc480d0cfd73"
    }
   },
   "outputs": [],
   "source": [
    "#here we merging the train ,features,store dataframe together\n",
    "train_merge=sales_train.merge(store,how='left')\n",
    "train=train_merge.merge(feat,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "a7493d20-8ee0-4baf-bd08-76506fd395fa"
    }
   },
   "outputs": [],
   "source": [
    "#here we merging the test,features,store dataframe together\n",
    "test_merge=test.merge(store,how='left')  \n",
    "test=test_merge.merge(feat,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "05168aa4-8e18-4613-ad59-26c1c555c930"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined merged_df shape:(536634, 17)\n"
     ]
    }
   ],
   "source": [
    "# Concatenating train & test\n",
    "train['train_or_test'] = 'train'\n",
    "test['train_or_test'] = 'test'\n",
    "merged_datframe = pd.concat([train,test], sort=False)\n",
    "print('Combined merged_df shape:{}'.format(merged_datframe.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Type', 'Size',\n",
       "       'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n",
       "       'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'train_or_test'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_datframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "13dad8a5-ce9d-4c6a-b137-ce5189ade725"
    }
   },
   "outputs": [],
   "source": [
    "# here we are Sorting the dataframe by store then Dept then date\n",
    "merged_datframe = merged_datframe.sort_values(by=['Store','Dept','Date'], axis=0).reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use One-hot-encode for  \"Type\" categorical variables \n",
    "merged_datframe = pd.get_dummies(merged_datframe, columns=['Type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use Label-encoder for  \"IsHoliday\" categorical variables \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "merged_datframe['IsHoliday'] = le.fit_transform(merged_datframe['IsHoliday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are Create new features ,their names are  - \"IsSuperbowl\", \"IsLaborday\", \"IsThanksgiving\", \"IsChristmas\" .in this feature we create four new columns in which we creating  some \n",
    "#special holidays like laborday,superbowl etc .if these dataes are available in our data then it will be 1 otherwise as 0\n",
    "\n",
    "def Superbowl(x):\n",
    "  if (x == '2010-02-12') | (x == '2011-02-11') | (x == '2012-02-10') | (x == '2013-02-08'):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def Laborday(x):\n",
    "  if (x == '2010-09-10') | (x == '2011-09-09') | (x == '2012-09-07') | (x == '2013-09-06'):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def Thanksgiving(x):\n",
    "  if (x == '2010-11-26') | (x == '2011-11-25') | (x == '2012-11-23') | (x == '2013-11-29'):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def Christmas(x):\n",
    "  if (x == '2010-12-31') | (x == '2011-12-30') | (x == '2012-12-28') | (x == '2013-12-27'):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "merged_datframe['IsSuperbowl'] = merged_datframe['Date'].apply(lambda x: Superbowl(x)) # adding the Issuperbowl feature\n",
    "merged_datframe['IsLaborday'] = merged_datframe['Date'].apply(lambda x: Laborday(x))\n",
    "merged_datframe['IsThanksgiving'] = merged_datframe['Date'].apply(lambda x: Thanksgiving(x))\n",
    "merged_datframe['IsChristmas'] = merged_datframe['Date'].apply(lambda x: Christmas(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from \"Date\" feature\n",
    "merged_datframe['WeekofMonth'] = merged_datframe['Date'].apply(lambda x: pendulum.parse(x).week_of_month) # here we are fetching week of month for date\n",
    "merged_datframe['Date'] = pd.to_datetime(merged_datframe['Date']) #Here we are converting the date column into datetime feature\n",
    "merged_datframe['Year'] = merged_datframe['Date'].dt.year         #Here we are extracting the date from  date column\n",
    "merged_datframe['Month'] = merged_datframe['Date'].dt.month       #Here we are extracting the month from  date column\n",
    "merged_datframe['Week'] = merged_datframe['Date'].dt.week         #Here we are extracting week from  date column\n",
    "merged_datframe['Day'] = merged_datframe['Date'].dt.day           #Here we are extracting day from  date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "8f7aa94c-48c6-441d-94a3-91ab79c13af0"
    }
   },
   "outputs": [],
   "source": [
    "# Features constructed from previous sales values\n",
    "#https://www.kaggle.com/abhilashawasthi/feature-engineering-lgb-model\n",
    "\n",
    "#this function will return the value of aggreagrate function that we pass like(mean ,minimum,maximum ,median of weekly sale)\n",
    "def create_sales_agg_monthwise_features(df, gpby_cols, target_col, agg_funcs):\n",
    "    gpby = df.groupby(gpby_cols)\n",
    "    newdf = df[gpby_cols].drop_duplicates().reset_index(drop=True)\n",
    "    for agg_name, agg_func in agg_funcs.items():\n",
    "        aggdf = gpby[target_col].agg(agg_func).reset_index()\n",
    "        aggdf.rename(columns={target_col:'Monthly_Sales_'+agg_name}, inplace=True)\n",
    "        newdf = newdf.merge(aggdf, on=gpby_cols, how='left')\n",
    "    return newdf\n",
    "\n",
    "# this function will return last 52 day monthly sale value for every rows\n",
    "def last_52_days(df, gpby_cols, target_col, lags):\n",
    "    gpby = df.groupby(gpby_cols)\n",
    "    for i in lags:\n",
    "    \n",
    "        df['_'.join([target_col, 'lag', str(i)])] = gpby[target_col].shift(i).values \n",
    "    return df\n",
    "\n",
    "##this function will return exponential mean of  last 52 day monthly sale value for every rows\n",
    "def exponential_mean_last_52_days(df, gpby_cols, target_col, alpha=[0.9], shift=[1]):\n",
    "    gpby = df.groupby(gpby_cols)\n",
    "    for a in alpha:\n",
    "        for s in shift:\n",
    "            df['_'.join([target_col, 'lag', str(s), 'ewm', str(a)])] = gpby[target_col].apply(lambda x: x.shift(s).ewm(alpha=0.95).mean())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sales lag features \n",
    "import numpy as np\n",
    "merged_new_datframe = last_52_days(merged_datframe, gpby_cols=['Store','Dept'], \n",
    "                                       target_col='Weekly_Sales', lags=[52])\n",
    "\n",
    "# creating ewm features \n",
    "merged_new_datframe= exponential_mean_last_52_days(merged_new_datframe, gpby_cols=['Store','Dept'], \n",
    "                                       target_col='Weekly_Sales', alpha=[0.95, 0.9, 0.8, 0.7, 0.6, 0.5],\n",
    "                                       shift=[52])\n",
    "\n",
    "# creating sales monthwise aggregated values\n",
    "df= create_sales_agg_monthwise_features(merged_new_datframe.loc[~(merged_new_datframe.train_or_test=='test'), :], gpby_cols=['Store','Dept', 'Month'], \n",
    "                                               target_col='Weekly_Sales', \n",
    "                                             agg_funcs={'mean':np.mean, \n",
    "                                              'median':np.median, 'max':np.max, \n",
    "                                              'min':np.min, 'std':np.std})\n",
    "# # Joining agg_df with merged_df_new\n",
    "merged_new_datframe = merged_new_datframe.merge(df, on=['Store','Dept', 'Month'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbpresent": {
     "id": "fd4c5235-8511-40bc-a684-4337f7b531a8"
    }
   },
   "outputs": [],
   "source": [
    "#here we are dropping the unnecessary columns\n",
    "merged_datframe = merged_datframe.drop(['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:(421570, 40), Test shape:(115064, 40)\n"
     ]
    }
   ],
   "source": [
    "#now we are seperating the train and test data\n",
    "# Final train and test datasets\n",
    "train = merged_new_datframe.loc[merged_new_datframe.train_or_test=='train', :]\n",
    "test = merged_new_datframe.loc[merged_new_datframe.train_or_test=='test', :]\n",
    "print('Train shape:{}, Test shape:{}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the date and train_test columns from the train dataset\n",
    "train = train.drop(['Date', 'train_or_test'], axis=1).dropna() \n",
    "\n",
    "#dropping the date and train_test and weekly_sales columns from the train dataset and filling the empty rows with 0\n",
    "\n",
    "test = test.drop(['Date', 'train_or_test', 'Weekly_Sales'], axis=1) .fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let store the cleaned train and test dataset in to csv file in directory\n",
    "train.to_csv('train_final.csv',index=False)\n",
    "test.to_csv('test_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train_final.csv')\n",
    "test=pd.read_csv('test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261073, 33)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115064, 32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Finding store and departement number that present on test data but not in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_store_dept=train[['Store','Dept']]\n",
    "test_store_dept=test[['Store','Dept']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07601c463044d8e928f99f47cc2a731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=261073.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm.notebook as tq\n",
    "store_dept=[]\n",
    "for i,j in tq.tqdm(train_store_dept.values):\n",
    "    store_dept.append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa192a5ec4764c1898a009826caee263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=115064.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "different_store_dept=[]\n",
    "for i,j in tq.tqdm(test_store_dept.values):\n",
    "  if (i,j) not in store_dept:\n",
    "    different_store_dep.append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "different_store_dept=list(set(c))\n",
    "len(different_store_dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, 99),\n",
       " (16, 47),\n",
       " (37, 24),\n",
       " (37, 33),\n",
       " (20, 47),\n",
       " (31, 47),\n",
       " (4, 39),\n",
       " (9, 99),\n",
       " (19, 39),\n",
       " (7, 19),\n",
       " (13, 99),\n",
       " (24, 99),\n",
       " (33, 56),\n",
       " (39, 99),\n",
       " (37, 26),\n",
       " (38, 55),\n",
       " (37, 99),\n",
       " (43, 23),\n",
       " (44, 22),\n",
       " (35, 19),\n",
       " (33, 31),\n",
       " (41, 99),\n",
       " (8, 96),\n",
       " (39, 19),\n",
       " (1, 99),\n",
       " (10, 47),\n",
       " (33, 49),\n",
       " (44, 49),\n",
       " (43, 71),\n",
       " (14, 99),\n",
       " (12, 47),\n",
       " (23, 47),\n",
       " (44, 24),\n",
       " (44, 33),\n",
       " (27, 47),\n",
       " (16, 99),\n",
       " (25, 47),\n",
       " (43, 55),\n",
       " (5, 80),\n",
       " (20, 99),\n",
       " (22, 96),\n",
       " (26, 48),\n",
       " (29, 47),\n",
       " (40, 47),\n",
       " (33, 72),\n",
       " (4, 45),\n",
       " (24, 78),\n",
       " (30, 27),\n",
       " (44, 26),\n",
       " (36, 52),\n",
       " (33, 99),\n",
       " (8, 45),\n",
       " (33, 44),\n",
       " (35, 96),\n",
       " (36, 6),\n",
       " (44, 44),\n",
       " (2, 47),\n",
       " (38, 34),\n",
       " (6, 47),\n",
       " (29, 49),\n",
       " (9, 80),\n",
       " (16, 58),\n",
       " (34, 39),\n",
       " (42, 22),\n",
       " (45, 39),\n",
       " (4, 47),\n",
       " (30, 29),\n",
       " (2, 77),\n",
       " (17, 96),\n",
       " (37, 44),\n",
       " (10, 99),\n",
       " (8, 47),\n",
       " (19, 47),\n",
       " (42, 49),\n",
       " (37, 71),\n",
       " (42, 6),\n",
       " (23, 99),\n",
       " (12, 99),\n",
       " (14, 96),\n",
       " (18, 48),\n",
       " (21, 47),\n",
       " (30, 22),\n",
       " (42, 24),\n",
       " (27, 99),\n",
       " (42, 33),\n",
       " (36, 56),\n",
       " (37, 55),\n",
       " (30, 49),\n",
       " (25, 99),\n",
       " (31, 78),\n",
       " (38, 29),\n",
       " (3, 80),\n",
       " (20, 96),\n",
       " (18, 96),\n",
       " (29, 99),\n",
       " (40, 99),\n",
       " (36, 22),\n",
       " (42, 72),\n",
       " (36, 31),\n",
       " (30, 24),\n",
       " (33, 23),\n",
       " (42, 26),\n",
       " (25, 19),\n",
       " (30, 33),\n",
       " (36, 49),\n",
       " (5, 98),\n",
       " (38, 22),\n",
       " (40, 19),\n",
       " (2, 99),\n",
       " (11, 47),\n",
       " (6, 99),\n",
       " (15, 47),\n",
       " (26, 47),\n",
       " (38, 49),\n",
       " (18, 43),\n",
       " (36, 24),\n",
       " (33, 71),\n",
       " (36, 33),\n",
       " (4, 99),\n",
       " (37, 32),\n",
       " (30, 26),\n",
       " (34, 45),\n",
       " (45, 45),\n",
       " (8, 99),\n",
       " (17, 47),\n",
       " (9, 98),\n",
       " (19, 99),\n",
       " (28, 47),\n",
       " (30, 44),\n",
       " (30, 99),\n",
       " (38, 24),\n",
       " (39, 77),\n",
       " (38, 33),\n",
       " (32, 47),\n",
       " (24, 43),\n",
       " (37, 49),\n",
       " (36, 72),\n",
       " (44, 55),\n",
       " (16, 48),\n",
       " (18, 45),\n",
       " (21, 99),\n",
       " (27, 78),\n",
       " (19, 19),\n",
       " (36, 99),\n",
       " (34, 47),\n",
       " (36, 44),\n",
       " (42, 30),\n",
       " (45, 47),\n",
       " (43, 22),\n",
       " (5, 47),\n",
       " (9, 45),\n",
       " (43, 49),\n",
       " (38, 99),\n",
       " (7, 47),\n",
       " (18, 47),\n",
       " (22, 47),\n",
       " (3, 98),\n",
       " (3, 47),\n",
       " (14, 43),\n",
       " (43, 33),\n",
       " (11, 99),\n",
       " (9, 47),\n",
       " (33, 32),\n",
       " (20, 77),\n",
       " (42, 44),\n",
       " (15, 99),\n",
       " (13, 47),\n",
       " (24, 47),\n",
       " (26, 99),\n",
       " (35, 47),\n",
       " (36, 30),\n",
       " (37, 29),\n",
       " (30, 23),\n",
       " (22, 49),\n",
       " (17, 99),\n",
       " (28, 99),\n",
       " (43, 26),\n",
       " (32, 99),\n",
       " (41, 47),\n",
       " (15, 37),\n",
       " (42, 55),\n",
       " (1, 47),\n",
       " (25, 48),\n",
       " (36, 23),\n",
       " (37, 22),\n",
       " (36, 32),\n",
       " (29, 48),\n",
       " (20, 45),\n",
       " (12, 96),\n",
       " (31, 45),\n",
       " (34, 99),\n",
       " (5, 99),\n",
       " (14, 47)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "different_store_dept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we observe in data that there are some store and departement feature that are present in test dataset but not present in train dataset like num\n",
    "5-99, 9-99, 10-99, 18-43, 24-43, 25-99, 34-39, 36-30, 37-29, 42-30, 45-39 etc\\\n",
    "there are total 193 store and departemnt ,  these store departement pair features that are present in test dataset but not in train dataset \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Modelling</h1>\n",
    "\n",
    "\n",
    "\n",
    "For making modelling parts we need to take care of some points that we mention below:\n",
    "*\n",
    "we are using random search_cv for parameter tunning\n",
    "*   \n",
    "As we know in random search cv if data is less, then it will fail that's why we need to take care of that .in our train dataset some store +departement pair rows are less then 5 that's why here we are writing condition if training data is geater then 5 then we will use randomsearch cv else we will use simple fit train data in modelling\n",
    "* Those store+departement pair that are present in test dataset but not present in train dataset we will return simple mean of that departemet belong  from train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model fit by store+departemnt </h1>\n",
    "\n",
    "\n",
    "In this model , train data will fit by store+dept combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to fit models by store+dept combo\n",
    "class Models_store_dept(object):\n",
    "    def __init__(self, clf, seed=0, params):\n",
    "        self.clf = clf()\n",
    "        self.params = params\n",
    "\n",
    "    def models_predict(self, train, test):\n",
    "        prediction = []\n",
    "        # this model is trained on store+dept filtered data\n",
    "        store_list = test[['Store',\t'Dept']].drop_duplicates().values.tolist()# in this model we are trained our model uniques store and dept one by one and predict the test dataset\n",
    "\n",
    "        for Store, Dept in tqdm(store_list):\n",
    "          x_train = train[(train['Store'] == Store) & (train['Dept'] == Dept)].drop(['Weekly_Sales'], axis=1)\n",
    "          y_train = train[(train['Store'] == Store) & (train['Dept'] == Dept)]['Weekly_Sales']\n",
    "          x_test = test[(test['Store'] == Store) & (test['Dept'] == Dept)]\n",
    "        \n",
    "          \n",
    "          if len(x_train) >= 5:#if length of data is greater then 5 then fit the model with best parameter\n",
    "          \n",
    "            # random search of parameters, using 5 fold cross validation\n",
    "            models = RandomizedSearchCV(estimator=self.clf, param_distributions = self.params, scoring='neg_root_mean_squared_error', \n",
    "                                     iid=False, n_jobs=-1, n_iter = 10, cv=5, verbose=5)\n",
    "            models = models.fit(x_train, y_train)\n",
    "\n",
    "            # validate the best model with optimized number of estimators\n",
    "            models = models.best_estimator_.fit(x_train, y_train)\n",
    "\n",
    "            # predict values\n",
    "            predict_test = models.predict(x_test)\n",
    "\n",
    "          elif len(x_train) >= 1: #if length of data is between 1 and 5 it will not do  parameter tunning ,it will normally fit\n",
    "            # fit model\n",
    "            models = self.clf.fit(x_train, y_train)\n",
    "\n",
    "            # predict values\n",
    "            predict_test = models.predict(x_test)\n",
    "\n",
    "          else:#if store +depart pair is not in train data but in test data it will return average of departement\n",
    "            predict_test = np.repeat(np.average(sales_train[sales_train['Dept'] == Dept]['Weekly_Sales']), len(x_test))\n",
    "          \n",
    "          # store the predicted values\n",
    "          prediction.extend(predict_test)\n",
    "\n",
    "        return prediction,models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4733c8ec-899e-47fe-8f11-cfb38462522d"
    }
   },
   "outputs": [],
   "source": [
    "#this function will create csv file of predicting values\n",
    "def csv(pred_values, file_name):\n",
    "  predict_dataframe = pd.DataFrame()\n",
    "  predict_dataframe['Id'] = sales_test['Store'].astype(str) + '_' +  sales_test['Dept'].astype(str) + '_' +  sales_test['Date'].astype(str)\n",
    "  predict_dataframe['Weekly_Sales'] = pred_values\n",
    "  return predict_dataframe.to_csv(os.path.join(file_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Xgboost model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting parameters and model\n",
    "xgboost_param = {'learning_rate': [0.01, 0.1, 1], #xgboost parameters\n",
    "              'max_depth': [4, 6, 8],\n",
    "              'subsample': [0.7, 0.9],\n",
    "              'gamma': [1],\n",
    "              'colsample_bytree' :[0.9, 1] }\n",
    "\n",
    "xgb = Models_store_dept(clf=XGBRegressor, seed=0, params=xgboost_param)#initialise model and our model should be xgb\n",
    "xgb_pred ,xgb_model= xgb.models_predict(train, test)#fitting train and test dataset in model\n",
    "csv(xgb_pred, 'output_xgb.csv')  #storing prediction in csv file\n",
    "\n",
    "\n",
    "# saving our model\n",
    "file_name = \"xgb.pkl\"\n",
    "\n",
    "pickle.dump(xgb_model, open(file_name, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Submission score of xgb model</h3>\n",
    "\n",
    "![title](xgb.png)\n",
    "\n",
    "Here we can see our kaggle score of xgb model is 2941 which is near top 10% score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Light GBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light GBM parameters\n",
    "lgbm_params = {'boosting_type': ['gbdt'],\n",
    "                'num_leaves': [10, 20], \n",
    "                'max_depth': [7, 14, 21],  \n",
    "                'learning_rate': [0.001, 0.01, 0.1],\n",
    "                'n_estimators': [100, 200], \n",
    "                'objective': ['regression']}\n",
    "\n",
    "\n",
    "lgbm = Models_store_dept(clf=lightgbm.LGBMRegressor, seed=0, params=lgbm_params)\n",
    "lgbm_pred ,lgbm_model= lgbm.models_predict(train, test)\n",
    "write_csv(lgbm_pred, 'output_StoreDept_lgbm.csv')\n",
    "\n",
    "file_name = \"lgbm.pkl\"\n",
    "\n",
    "pickle.dump(lgbm_model, open(file_name, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Submission score of Light GBM model</h3>\n",
    "\n",
    "![title](lgbm.png)\n",
    "\n",
    "Here we can see our kaggle score of random forest model is 3830 which is far from top 10% score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Gradient boost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradient Boosting parameters and model\n",
    "gb_param = {'max_depth': [3],\n",
    "              'n_estimators': [5],\n",
    "              'learning_rate': [0.1]}\n",
    "\n",
    "\n",
    "gbm = Models_store_dept(clf=GradientBoostingRegressor, seed=0, params=gb_param)#initialise model and our model should be gradient booster regressor\n",
    "gbm_pred,gbm_model = gbm.models_predict(train, test)#fitting train and test dataset in model\n",
    "write_csv(gbm_pred, 'output_gbm.csv')#storing prediction in csv file\n",
    "\n",
    "# saving our gradient booster regressor model\n",
    "file_name = \"gbm.pkl\"\n",
    "\n",
    "pickle.dump(gbm_model, open(file_name, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Submission score of Gradient boost model</h3>\n",
    "\n",
    "![title](gbm.png)\n",
    "\n",
    "Here we can see our kaggle score of Gradient boost model is 2888 which is neare to top 10% score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1efef705-e5fb-4cf3-b19e-ba7f30429261"
    }
   },
   "source": [
    "<h1>Prophet Model</h1>\n",
    "\n",
    "\n",
    " Prophet is open source software released by Facebook’s Core Data Science team.\n",
    "Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "nbpresent": {
     "id": "0d92faac-35fd-4b75-b378-cb160a14586d"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3342/3342 [03:26<00:00, 16.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from fbprophet import Prophet\n",
    "from tqdm import tqdm\n",
    "\n",
    "prop_df = merged_new_datframe[['Store',\t'Dept',\t'Date',\t'Weekly_Sales', 'train_or_test']]\n",
    "prop_df = prop_df.set_index('Date')\n",
    "prop_store_dept = prop_df[['Store', 'Dept']].drop_duplicates().reset_index(drop=True).values.tolist()\n",
    "\n",
    "new = []\n",
    "idx = pd.date_range('2/5/2010', '10/26/2012', freq='W-FRI')\n",
    "\n",
    "for store, dept in tqdm(prop_store_dept):\n",
    "  numeric = prop_df[(prop_df['Store'] == store) & (prop_df['Dept'] == dept) & (prop_df['train_or_test'] == 'train')].reindex(idx)\n",
    "  values = {'Store': store, 'Dept': dept, 'Weekly_Sales': 0, 'train_or_test': 'train'}\n",
    "  numeric_ts = numeric.fillna(value=values)\n",
    "  \n",
    "  new.append(numeric_ts)\n",
    "  new.append(prop_df[(prop_df['Store'] == store) & (prop_df['Dept'] == dept) & (prop_df['train_or_test'] == 'test')])\n",
    "\n",
    "pro_new_df = pd.concat(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "nbpresent": {
     "id": "3b4d5c35-2ebf-457c-8240-81155101df5c"
    }
   },
   "outputs": [],
   "source": [
    "#creating dataframe of holidays for prophet model\n",
    "Superbowl = pd.DataFrame({\n",
    "  'holiday' : 'superbowl',\n",
    "  'ds' : pd.to_datetime(['2010-02-12','2011-02-11','2012-02-10','2013-02-08'])\n",
    "  })\n",
    "Laborday = pd.DataFrame({\n",
    "  'holiday' : 'laborday',\n",
    "  'ds' : pd.to_datetime(['2010-09-10','2011-09-09','2012-09-07','2013-09-06'])\n",
    "  })\n",
    "\n",
    "Thanksgiving = pd.DataFrame({\n",
    "  'holiday' : 'thanksgiving',\n",
    "  'ds' : pd.to_datetime(['2010-11-26','2011-11-25','2012-11-23','2013-11-29'])\n",
    "  })\n",
    "\n",
    "Christmas = pd.DataFrame({\n",
    "  'holiday' : 'christmas',\n",
    "  'ds' : pd.to_datetime(['2010-12-31','2011-12-30','2012-12-28','2013-12-27'])\n",
    "  })\n",
    "\n",
    "holidays = pd.concat((Superbowl, Laborday, Thanksgiving, Christmas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "pro_new_copy = pro_new_df.copy()\n",
    "pro_new_copy = pro_new_copy.reset_index().rename(columns = {'index':'Date'})\n",
    "prop _forecast = []\n",
    "store_dept = test[['Store',\t'Dept']].drop_duplicates().values.tolist()\n",
    "\n",
    "for store, dept in tqdm(store_dept):\n",
    "  \n",
    " # print ('Store No: ', store, ' Dept No:', dept)\n",
    "\n",
    "  # Lets narrow our analysis for a single store+dept combo \n",
    "  df = pro_new_copy[(pro_new_copy['Store'] == store) &  (pro_new_copy['Dept'] == dept) & (pro_new_copy['train_or_test'] == 'train')][['Date', 'Weekly_Sales']]\n",
    "\n",
    "  # Prophet only takes data as a dataframe with a ds (datestamp) and y (value we want to forecast) column. \n",
    "  df = df.rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'})\n",
    "\n",
    "  if len(df) >= 5:\n",
    "    # Create an instance of the Prophet class and then fit our dataframe to it.\n",
    "    prophet = Prophet(holidays=holidays)\n",
    "    prophet.fit(df)\n",
    "\n",
    "    # Create a dataframe with the dates for which we want a prediction to be made with make_future_dataframe(). \n",
    "    # Then specify the number of days to forecast using the periods parameter.\n",
    "    df_forecast = prophet.make_future_dataframe(periods=39, freq='W-FRI')\n",
    "\n",
    "    # Call predict to make a prediction and store it in the forecast dataframe.\n",
    "    df_forecast = prophet.predict(df_forecast)\n",
    "\n",
    "    # Retrieve predictions for the dates in the test set\n",
    "    df_forecast = pd.merge(pro_new_copy[(pro_new_copy['Store'] == store) &  (pro_new_copy['Dept'] == dept) & (pro_new_copy['train_or_test'] == 'test')][['Store', 'Dept', 'Date']], \n",
    "                           df_forecast[['ds', 'yhat']], \n",
    "                           left_on='Date', right_on='ds', how = 'left')[['Store', 'Dept', 'Date', 'yhat']]\n",
    "    \n",
    "    \n",
    "  else:\n",
    "\n",
    "    forecast = np.repeat(np.average(pro_new_copy[(pro_new_copy['Dept'] == dept) & (pro_new_copy['train_or_test'] == 'train')]['Weekly_Sales']), \n",
    "                         len(pro_new_copy[(pro_new_copy['Store'] == store) &  (pro_new_copy['Dept'] == dept) & (pro_new_copy['train_or_test'] == 'test')]))\n",
    "\n",
    "    df_forecast = pro_new_copy[(pro_new_copy['Store'] == store) &  (pro_new_copy['Dept'] == dept) & (pro_new_copy['train_or_test'] == 'test')][['Store', 'Dept', 'Date']]\n",
    "\n",
    "    df_forecast['yhat'] = forecast\n",
    "\n",
    "  prop _forecast.append(df_forecast)\n",
    "\n",
    "prop _forecast = pd.concat(prop _forecast)\n",
    "prop _forecast['Store'] = prop _forecast['Store'].astype(int)\n",
    "prop _forecast['Dept'] = prop _forecast['Dept'].astype(int)\n",
    "prop _forecast['Id'] = prop _forecast['Store'].astype(str) + '_' +  prop _forecast['Dept'].astype(str) + '_' +  prop _forecast['Date'].astype(str)\n",
    "prop _forecast = prop _forecast.rename(columns = {'yhat': 'Weekly_Sales'})\n",
    "prop _forecast = prop _forecast[['Id', 'Weekly_Sales']]\n",
    "prop _forecast.to_csv(os.path.join( 'Output_Prohpet.csv'), index=False)\n",
    "\n",
    "\n",
    "file_name = \"porphet.pkl\"\n",
    "pickle.dump(prophet, open(file_name, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Submission score of Prophet model</h3>\n",
    "\n",
    "![title](pro.png)\n",
    "\n",
    "Here we can see our kaggle score of Prophet model is 2928 which is nearer to top 10% score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a195223f-a206-415a-8c34-27722982241f"
    }
   },
   "source": [
    "<h1>average model (Ensemble Model)</h1>\n",
    "\n",
    "\n",
    "In this model we take the average prediction of xgb,gbm,prophet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "cf0accf8-d03f-491e-9d9f-556dd7e34acb"
    }
   },
   "outputs": [],
   "source": [
    "def AvgModels(*args):\n",
    "  predictions = np.column_stack(args)\n",
    "  return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "95cd00dc-2322-40de-9c3a-8e6c12b4667e"
    }
   },
   "outputs": [],
   "source": [
    "data1=pd.read_csv('Output_Prohpet.csv')\n",
    "data2=pd.read_csv('output_xgb.csv')\n",
    "data3=pd.read_csv('output_StoreDept_gbm.csv')\n",
    "dat1=data1['Weekly_Sales']\n",
    "dat3=data2['Weekly_Sales']\n",
    "dat4=data3['Weekly_Sales']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "15ff1f41-11d3-46e9-a416-9c48674bd98c"
    }
   },
   "outputs": [],
   "source": [
    "result=AvgModels(dat1,dat3,dat4)\n",
    "csv(result, 'output_dept_result3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Submission score of Average model (Ensemble Model)</h3>\n",
    "\n",
    "![title](final.png)\n",
    "\n",
    "Here we can see our kaggle score of Average model (Ensemble Model) is 2652 which is under top 10% score.Hence ensemble model is best model for our prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Final Observation</h1>\n",
    "\n",
    "\n",
    "\n",
    "After applying xgb model we got acc,gbm,light gbm,prophet model we got accuracy as mentiob below in table in which we can see ensemble model is best model for our prediction,Because by using an ensemble model we get our accuracy under top 10% kaggle score<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Model         | Kaggle_score | Under 10% Score |\n",
    "| ---           | ---          | ---             |\n",
    "| XGB_regressor | 2941 | No|\n",
    "| GBM Model     | 2888 |No|\n",
    "| Light GBM     | 3830 |No|\n",
    "| Prophet       | 2928 |No|\n",
    "| Ensemble      | 2652 |Yes|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "2a15aecd-d28e-482a-947a-b07085614065",
    "theme": {
     "2a15aecd-d28e-482a-947a-b07085614065": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "2a15aecd-d28e-482a-947a-b07085614065",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
